{
 "metadata": {
  "toc-autonumbering": true,
  "kernelspec": {
   "name": "glue_pyspark",
   "display_name": "Glue PySpark",
   "language": "python"
  },
  "language_info": {
   "name": "Python_Glue_Session",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "pygments_lexer": "python3",
   "file_extension": ".py"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# AWS Glue Studio Notebook\n",
    "\n",
    "**You are now running a AWS Glue Studio notebook; To start using your notebook you need to start an AWS Glue Interactive Session.**\n",
    "\n",
    "**Optional: Run this cell to see available notebook commands (\"magics\")**."
   ],
   "metadata": {
    "editable": true,
    "trusted": true
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%help"
   ],
   "metadata": {
    "editable": true,
    "trusted": true
   },
   "execution_count": 11,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/markdown": "\n# Available Magic Commands\n\n## Sessions Magic\n\n----\n    %help                             Return a list of descriptions and input types for all magic commands. \n    %profile            String        Specify a profile in your aws configuration to use as the credentials provider.\n    %region             String        Specify the AWS region in which to initialize a session. \n                                      Default from ~/.aws/config on Linux or macOS, \n                                      or C:\\Users\\ USERNAME \\.aws\\config\" on Windows.\n    %idle_timeout       Int           The number of minutes of inactivity after which a session will timeout. \n                                      Default: 2880 minutes (48 hours).\n    %session_id_prefix  String        Define a String that will precede all session IDs in the format \n                                      [session_id_prefix]-[session_id]. If a session ID is not provided,\n                                      a random UUID will be generated.\n    %status                           Returns the status of the current Glue session including its duration, \n                                      configuration and executing user / role.\n    %session_id                       Returns the session ID for the running session. \n    %list_sessions                    Lists all currently running sessions by ID.\n    %stop_session                     Stops the current session.\n    %glue_version       String        The version of Glue to be used by this session. \n                                      Currently, the only valid options are 2.0 and 3.0. \n                                      Default: 2.0.\n----\n\n## Selecting Job Types\n\n----\n    %streaming          String        Sets the session type to Glue Streaming.\n    %etl                String        Sets the session type to Glue ETL.\n    %glue_ray           String        Sets the session type to Glue Ray.\n----\n\n## Glue Config Magic \n*(common across all job types)*\n\n----\n\n    %%configure         Dictionary    A json-formatted dictionary consisting of all configuration parameters for \n                                      a session. Each parameter can be specified here or through individual magics.\n    %iam_role           String        Specify an IAM role ARN to execute your session with.\n                                      Default from ~/.aws/config on Linux or macOS, \n                                      or C:\\Users\\%USERNAME%\\.aws\\config` on Windows.\n    %number_of_workers  int           The number of workers of a defined worker_type that are allocated \n                                      when a session runs.\n                                      Default: 5.\n    %additional_python_modules  List  Comma separated list of additional Python modules to include in your cluster \n                                      (can be from Pypi or S3).\n----\n\n                                      \n## Magic for Spark Jobs (ETL & Streaming)\n\n----\n    %worker_type        String        Set the type of instances the session will use as workers. \n                                      ETL and Streaming support G.1X, G.2X, G.4X and G.8X. \n                                      Default: G.1X.\n    %connections        List          Specify a comma separated list of connections to use in the session.\n    %extra_py_files     List          Comma separated list of additional Python files From S3.\n    %extra_jars         List          Comma separated list of additional Jars to include in the cluster.\n    %spark_conf         String        Specify custom spark configurations for your session. \n                                      E.g. %spark_conf spark.serializer=org.apache.spark.serializer.KryoSerializer\n----\n                                      \n## Magic for Ray Job\n\n----\n    %min_workers        Int           The minimum number of workers that are allocated to a Ray job. \n                                      Default: 1.\n    %object_memory_head Int           The percentage of free memory on the instance head node after a warm start. \n                                      Minimum: 0. Maximum: 100.\n    %object_memory_worker Int         The percentage of free memory on the instance worker nodes after a warm start. \n                                      Minimum: 0. Maximum: 100.\n----\n\n## Action Magic\n\n----\n\n    %%sql               String        Run SQL code. All lines after the initial %%sql magic will be passed\n                                      as part of the SQL code.  \n----\n\n"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "##  Run this cell to set up and start your interactive session."
   ],
   "metadata": {
    "editable": true,
    "trusted": true
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%idle_timeout 2880\n",
    "%glue_version 4.0\n",
    "%worker_type G.1X\n",
    "%number_of_workers 2\n",
    "%extra_py_files s3://807388292768-us-east-1-artifacts/versioned-artifacts/simple_glue-sbx_extra_py_files/LATEST\n",
    "%additional_python_modules s3pathlib==2.0.1"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": 18,
   "outputs": [
    {
     "name": "stderr",
     "text": "You are already connected to a glueetl session 387e51cd-ab65-470b-8c80-a7946bf286da.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Current idle_timeout is 2880 minutes.\nidle_timeout has been set to 2880 minutes.\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "You are already connected to a glueetl session 387e51cd-ab65-470b-8c80-a7946bf286da.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Setting Glue version to: 4.0\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "You are already connected to a glueetl session 387e51cd-ab65-470b-8c80-a7946bf286da.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Previous worker type: G.1X\nSetting new worker type to: G.1X\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "You are already connected to a glueetl session 387e51cd-ab65-470b-8c80-a7946bf286da.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Previous number of workers: 2\nSetting new number of workers to: 2\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "You are already connected to a glueetl session 387e51cd-ab65-470b-8c80-a7946bf286da.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Extra py files to be included:\ns3://807388292768-us-east-1-artifacts/versioned-artifacts/simple_glue-sbx_extra_py_files/LATEST\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "You are already connected to a glueetl session 387e51cd-ab65-470b-8c80-a7946bf286da.\n\nNo change will be made to the current session that is set as glueetl. The session configuration change will apply to newly created sessions.\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Additional python modules to be included:\ns3pathlib==2.0.1\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Glue Job Script\n",
    "\n",
    "## Prepare"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "This is a sample Glue Job to demonstrate:\n",
    "\n",
    "- Glue job scripting best practice\n",
    "    - Parameter management\n",
    "    - Fast development in interactive Glue Jupyter Notebook\n",
    "- Glue job unit test best practice\n",
    "- Glue job integration test best practice\n",
    "\"\"\"\n",
    "\n",
    "# standard library\n",
    "import typing as T\n",
    "import sys\n",
    "import os\n",
    "import dataclasses\n",
    "from pprint import pprint\n",
    "\n",
    "# third party library\n",
    "from s3pathlib import S3Path\n",
    "\n",
    "# pyspark and glue stuff\n",
    "from pyspark.context import SparkContext\n",
    "\n",
    "from awsglue.dynamicframe import DynamicFrame\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "from awsglue.utils import getResolvedOptions\n",
    "\n",
    "# custom library\n",
    "from simple_glue.glue_libs.glue_utils import double_a_column"
   ],
   "metadata": {
    "editable": true,
    "trusted": true
   },
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "text": "\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Identify Run Mode\n",
    "\n",
    "In the lifecycle of a Glue ETL script, from development to testing and production, various requirements and scenarios arise. These requirements can differ based on the specific stage of development. Let's explore some examples:\n",
    "\n",
    "**Development Stage**:\n",
    "\n",
    "- In the initial development phase using Jupyter Notebook, it is often beneficial to connect the ETL script to a smaller dataset to facilitate faster execution time. Additionally, overriding input parameters can aid in the debugging process.\n",
    "- To enhance readability and maintainability, it may be helpful to include additional information or code snippets within the script.\n",
    "\n",
    "**Unit Testing Stage**:\n",
    "\n",
    "- During unit testing in the Glue container image within the CI runtime, the focus may be on testing the transformation logic rather than the entire ETL logics. In such cases, it might be preferable to skip the \"read the data\" and \"write the data\" phases and only import the relevant portions of the code.\n",
    "\n",
    "**Production Stage**:\n",
    "\n",
    "- In a production environment, debug and logging logic may not be required. And we should use the real production data to run the ETL logics.\n",
    "\n",
    "While the ETL logic remains mostly the same, there are slight differences depending on the runtime environment. Maintaining multiple copies of the same Glue ETL script and keeping them in sync can be challenging.\n",
    "\n",
    "To address this issue, we have a small code snippet that defines some boolean flag variables to identify the runtime environment. With this approach, we can keep the majority of the ETL logic consistent across all stages while making slight modifications when necessary. Consequently, we only need to maintain a single script, providing the flexibility to customize the ETL logic based on the runtime environment."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# identify the current run Mode\n",
    "IS_GLUE_JOB_RUN = False\n",
    "IS_DEV_NOTEBOOK = False\n",
    "IS_LOCAL = False\n",
    "\n",
    "# in real glue job runtime, it always has --JOB_RUN_ID argument\n",
    "if \"--JOB_RUN_ID\" in sys.argv:\n",
    "    IS_GLUE_JOB_RUN = True\n",
    "    print(\"Now we are on GLUE_JOB_RUN mode\")\n",
    "# in Jupyter Notebook glue job runtime, the $HOME is /home/spark\n",
    "elif os.environ[\"HOME\"] == \"/home/spark\":\n",
    "    IS_DEV_NOTEBOOK = True\n",
    "    print(\"Now we are on DEV_NOTEBOOK mode\")\n",
    "# otherwise, we assume that we are on local development or CI runtime for development or testing\n",
    "else:\n",
    "    IS_LOCAL = True\n",
    "    print(\"Now we are on IS_LOCAL mode\")\n",
    "\n",
    "# Sometimes you can force to run in DEV_NOTEBOOK mode in regular Glue job run for debugging\n",
    "# Also you can force to run in GLUE_JOB_RUN mode in your jupyter notebook, for example, testing the production data connection\n",
    "# ensure that you commented this out before committing the code\n",
    "\n",
    "# IS_GLUE_JOB_RUN = True\n",
    "# IS_DEV_NOTEBOOK = True"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "text": "Now we are on DEV_NOTEBOOK mode\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Parameter Management\n",
    "\n",
    "The AWS Glue official documentation gives an example of loading parameters from the Spark submit command line. \n",
    "\n",
    "```python\n",
    "args = getResolvedOptions(sys.argv, [\"JOB_NAME\"])\n",
    "param1 = args.get(\"param1\")\n",
    "param2 = args.get(\"param2\")\n",
    "param3 = args.get(\"param3\")\n",
    "```\n",
    "\n",
    "Now, this method works, but let's be honest—it's not the most elegant solution. You end up with a bunch of variables in your code, and it's hard to tell which ones come from the input parameter and which ones are derived from it. Plus, figuring out the input/output interface of your Glue script becomes a bit of a headache.\n",
    "\n",
    "But don't worry, we've got a solution! We've added a thin \"parameter management\" layer to your script. It's a data class that acts as a hub for all your ETL script parameters. The static attributes represent the Glue job parameters, while the dynamic methods calculate and give you the derived values. And here's the best part—you can define your own logic to load parameters from different places depending on the situation. Want to hard code some parameters while developing in Jupyter Notebook? Go for it! Need to load parameters from the AWS Parameter Store during production? No problem!\n",
    "\n",
    "By using this approach, you'll notice a huge improvement in the readability and maintainability of your ETL scripts. Plus, it takes the burden of parameter loading off your main ETL logic and keeps everything nice and organized in one place."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "@dataclasses.dataclass\n",
    "class Param:\n",
    "    s3uri_input: str\n",
    "    s3uri_output: str\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, args: T.Optional[T.Dict[str, str]]):\n",
    "        if IS_GLUE_JOB_RUN:\n",
    "            return cls(\n",
    "                s3uri_input=args.get(\"s3uri_input\"),\n",
    "                s3uri_output=args.get(\"s3uri_output\"),\n",
    "            )\n",
    "        else:\n",
    "            return cls(\n",
    "                s3uri_input=\"s3://807388292768-us-east-1-data/projects/simple_glue/dev/unittest/unnest/input/\",\n",
    "                s3uri_output=\"s3://807388292768-us-east-1-data/projects/simple_glue/dev/unittest/unnest/output/\",\n",
    "            )\n",
    "\n",
    "    @property\n",
    "    def s3dir_input(self) -> S3Path:\n",
    "        \"\"\"\n",
    "        The S3Path object version of the input S3 folder.\n",
    "        \"\"\"\n",
    "        return S3Path(self.s3uri_input)\n",
    "\n",
    "    @property\n",
    "    def s3dir_output(self) -> S3Path:\n",
    "        \"\"\"\n",
    "        The S3Path object version of the output S3 folder.\n",
    "        \"\"\"\n",
    "        return S3Path(self.s3uri_output)"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "text": "\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Utility Functions\n",
    "\n",
    "In order to enhance the maintainability of your ETL logic, it is advisable to consider breaking down the intermediate transformation steps into smaller Python functions with clear input and output definitions and documentations. These Python functions essentially serve as fundamental building blocks, which can be imported and individually tested during the unit testing phase. This modular approach enables you to validate the functionality of each function in isolation. Subsequently, once you have confidence in the reliability of these functions, you can seamlessly orchestrate them together to streamline your ETL logic. The result is a codebase that is organized and easier to manage."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def transform_data(gdf: DynamicFrame) -> DynamicFrame:\n",
    "    \"\"\"\n",
    "    unnest / flatten complicte object and double the value of the columhn ``details.value``.\n",
    "    \"\"\"\n",
    "    gdf_transformed = gdf.unnest(transformation_ctx=\"gdf_unnested\")\n",
    "    gdf_transformed = double_a_column(\n",
    "        gdf_transformed,\n",
    "        col_name=\"details.value\",\n",
    "        trans_ctx=\"double_a_column\"\n",
    "    )\n",
    "    return gdf_transformed"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "text": "\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Glue ETL Logics"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "class GlueETL:\n",
    "    def run(self):\n",
    "        self.step0_preprocess()\n",
    "        self.step1_read_data()\n",
    "        self.step2_transform_data()\n",
    "        self.step3_write_data()\n",
    "        self.step4_post_process()\n",
    "        return self.gdf_transformed\n",
    "\n",
    "    def step0_preprocess(self):\n",
    "        # print(\"--- sys.argv ---\")\n",
    "        # pprint(sys.argv)\n",
    "        # print(\"--- env vars ---\")\n",
    "        # for k, v in os.environ.items():\n",
    "        #     print(f\"{k}={v}\")\n",
    "\n",
    "        self.spark_ctx = SparkContext.getOrCreate()\n",
    "        self.glue_ctx = GlueContext(self.spark_ctx)\n",
    "        self.spark_ses = self.glue_ctx.spark_session\n",
    "\n",
    "        if IS_GLUE_JOB_RUN:\n",
    "            self.args = getResolvedOptions(\n",
    "                sys.argv,\n",
    "                [\n",
    "                    \"JOB_NAME\",\n",
    "                    \"s3uri_input\",\n",
    "                    \"s3uri_output\",\n",
    "                ]\n",
    "            )\n",
    "            self.job = Job(self.glue_ctx)\n",
    "            self.job.init(self.args[\"JOB_NAME\"], self.args)\n",
    "        else:\n",
    "            self.args = None\n",
    "            self.job = None\n",
    "\n",
    "        self.param = Param.load(self.args)\n",
    "\n",
    "    def step1_read_data(self):\n",
    "        self.gdf = self.glue_ctx.create_dynamic_frame.from_options(\n",
    "            connection_type=\"s3\",\n",
    "            connection_options=dict(\n",
    "                paths=[\n",
    "                    self.param.s3dir_input.uri,\n",
    "                ],\n",
    "                recurse=True,\n",
    "            ),\n",
    "            format=\"json\",\n",
    "            format_options=dict(multiLine=\"true\"),\n",
    "            transformation_ctx=\"read_gdf\",\n",
    "        )\n",
    "\n",
    "    def step2_transform_data(self):\n",
    "        self.gdf_transformed = transform_data(self.gdf)\n",
    "\n",
    "    def step3_write_data(self):\n",
    "        self.glue_ctx.write_dynamic_frame.from_options(\n",
    "            frame=self.gdf_transformed,\n",
    "            connection_type=\"s3\",\n",
    "            connection_options=dict(\n",
    "                path=self.param.s3dir_output.uri,\n",
    "            ),\n",
    "            format=\"parquet\",\n",
    "            transformation_ctx=\"write_gdf\",\n",
    "        )\n",
    "\n",
    "    def step4_post_process(self):\n",
    "        # only commit job in real Glue job run\n",
    "        if IS_GLUE_JOB_RUN:\n",
    "            self.job.commit()"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "text": "\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Entry Point API Call"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# If it is not LOCAL runtime, we should run the ETL logics\n",
    "if IS_LOCAL is False:\n",
    "    gdf_transformed = GlueETL().run()\n",
    "    if IS_DEV_NOTEBOOK:\n",
    "        gdf_transformed.toDF().show()"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "text": "+-------------+--------+------------+\n|details.value|event_id|details.name|\n+-------------+--------+------------+\n|            2|     e-1| measurement|\n|            4|     e-2| measurement|\n|           20|    e-10| measurement|\n|            6|     e-3| measurement|\n|           22|    e-11| measurement|\n|           24|    e-12| measurement|\n|           14|     e-7| measurement|\n|            8|     e-4| measurement|\n|           16|     e-8| measurement|\n|           10|     e-5| measurement|\n|           18|     e-9| measurement|\n|           12|     e-6| measurement|\n+-------------+--------+------------+\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}